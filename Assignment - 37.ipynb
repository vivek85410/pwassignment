{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f658004-96dd-4a8e-ba0e-b4834f947482",
   "metadata": {},
   "source": [
    "# Ques - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d1df9-185d-4ecf-a37a-fc98069eb956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Overfitting : it is the Error condition in the machine learning, where the model predicts\n",
    "    accurately on the training data but not in case of new/test data.\n",
    "    it has high varience and low bias.\n",
    "    \n",
    "    Underfitting : it is the Error condition in the machine learning, where the model predicts\n",
    "    wrong is both training as well as testing data.\n",
    "    it has high bias and  low varience.\n",
    "    \n",
    "    Consequence of overfitting and underfitting is that the model gives false prediction and \n",
    "    doesn't perform as expected.\n",
    "    \n",
    "    techniques to mitigate overfitting : \n",
    "    1. Increase training data.\n",
    "    2. Reduce model complexity\n",
    "    3. Early stopping during the training phase (have an eye over the loss over the training\n",
    "        period as soon as loss begins to increase stop training).\n",
    "    4. Ridge Regularization and Lasso Regularization.\n",
    "    5. Use dropout for neural networks to tackle overfitting\n",
    "    \n",
    "    techniques to mitigate underfitting : \n",
    "    1. Increase model complexity.\n",
    "    2. Increase the number of features, performing feature engineering.\n",
    "    3. Remove noise from the data.\n",
    "    4. Increase the number of epochs or increase the duration of training to get better results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ab887-d8c6-4707-b436-5774bd41b7af",
   "metadata": {},
   "source": [
    "# Ques - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b9385-64b0-4bb3-b5e1-5131dfff5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" To reduce overfitting : \n",
    "    1. Increase training data - As we increase the training data then all the problem of high\n",
    "        bias and low varience will get removed.\n",
    "        \n",
    "    2. Reduce model complexity - by reducing the model complexity the prediction of the model\n",
    "        will get linear and will predict correct output.\n",
    "        \n",
    "    3. Early stopping during the training phase (have an eye over the loss over the training\n",
    "        period as soon as loss begins to increase stop training) - Early stopping pauses the\n",
    "        training phase before the machine learning model learns the noise in the data. However\n",
    "        , getting the timing right is important; else the model will still not give accurate\n",
    "        results.\n",
    "        \n",
    "    4. Ridge Regularization and Lasso Regularization - Regularization is a collection of \n",
    "        training/optimization techniques that seek to reduce overfitting. These methods \n",
    "        try to eliminate those factors that do not impact the prediction outcomes by \n",
    "        grading features based on importance. \n",
    "        \n",
    "    5. Use dropout for neural networks to tackle overfitting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14a337-c3be-45e7-98f8-4543f7085e88",
   "metadata": {},
   "source": [
    "# Ques - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601461a-fd8b-49dd-8d8d-29692cdea919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Underfitting : it is the Error condition in the machine learning, where the model predicts\n",
    "    wrong is both training as well as testing data.\n",
    "    it has high bias and  low varience.\n",
    "    \n",
    "    scenarios where underfitting can happen in machine learning : \n",
    "    1. When the model is too simple, So it may be not capable to represent the complexities\n",
    "        in the data. \n",
    "    2. The input features which is used to train the model is not the adequate representations\n",
    "        of underlying factors influencing the target variable.\n",
    "    3. The size of the training dataset used is not enough.\n",
    "    4. Excessive regularization are used to prevent the overfitting, which constraint the \n",
    "        model to capture the data well.\n",
    "    5. Features are not scaled.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e94064-9ab5-4948-8aa1-8b5bdbba7904",
   "metadata": {},
   "source": [
    "# Ques - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef274d-66a9-44cc-b790-72b92e43cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Bias varience trade-off : \n",
    "    If the algorithm is too simple (hypothesis with linear equation) then it may be\n",
    "    on high bias and low variance condition and thus is error-prone. If algorithms\n",
    "    fit too complex (hypothesis with high degree equation) then it may be on high \n",
    "    variance and low bias.\n",
    "                        So, the bias varience trade off is In statistics and machine\n",
    "    learning, the biasâ€“variance tradeoff describes the relationship between a model's\n",
    "    complexity, the accuracy of its predictions, and how well it can make predictions\n",
    "    on previously unseen data that were not used to train the model.\n",
    "    \n",
    "    bias and varience can affect the model as:\n",
    "    when the bias is high and tha varience is low then the data is not regulated\n",
    "    and centralized and in this case there will be error of underfitting.\n",
    "    and when the bias is low and the varience is high then the data is centralized \n",
    "    but scattered from the centre and it will be the error of overfitting.\n",
    "    and when both the bias and varience is high then the data is scattered and also\n",
    "    not centralized.\n",
    "    but in case of low varience and low bias ,  the data will be centralized and \n",
    "    model eill perfectly predict the output otherwise model will not behave as expected.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599335e0-8145-4993-80c4-38ca51ada628",
   "metadata": {},
   "source": [
    "# Ques - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c27a2d-a6ca-492b-9a66-c5402d8430a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" some common method of detecting overfitting and underfitting is:\n",
    "    when our machine learning model predicts so much error in the output or when the \n",
    "    predicted value is different then the actual value then we can say that there is\n",
    "    some case of overfitting or underfitting.\n",
    "    \n",
    "    another method of overfitting is K-fold cross-validation.\n",
    "    \n",
    "    we can say that model is underfitted when the test value and the output predicted\n",
    "    values have so much errors.\n",
    "    and when the test value predicted is accurate but the output predicted value is \n",
    "    not accurate then there is error of overfitting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709afacf-ee76-470a-9fcc-55048a0b7ad6",
   "metadata": {},
   "source": [
    "# Ques - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8f8c9-329b-4ab2-8e17-2328bc088a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" diff between bias and varience : \n",
    "    Bias - When an algorithm is employed in a machine learning model and it does not\n",
    "    fit well, a phenomenon known as bias can develop. Bias arises in several situations.\n",
    "    The disparity between the values that were predicted and the values that were \n",
    "    actually observed is referred to as bias.\n",
    "    The model is incapable of locating patterns in the dataset that it was trained on,\n",
    "    and it produces inaccurate results for both seen and unseen data.\n",
    "    \n",
    "    Varience - The term \"variance\" refers to the degree of change that may be expected\n",
    "    in the estimation of the target function as a result of using multiple sets of\n",
    "    training data.\n",
    "    A random variable's variance is a measure of how much it varies from the value that\n",
    "    was predicted for it.\n",
    "    The model recognizes the majority of the dataset's patterns and can even learn from\n",
    "    the noise or data that isn't vital to its operation.\n",
    "    \n",
    "    Linear Regression - High bias , low varience\n",
    "    Decision Tree - low bias , high varience\n",
    "    Bagging - low bias , high varience\n",
    "    Random Forest - low bias , high varience\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933d088-ae3a-407a-a2fd-c40a56e7e261",
   "metadata": {},
   "source": [
    "# Ques - 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61add5da-7f71-4436-b7d7-0e9e8cdd8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Regularization refers to techniques that are used to calibrate machine learning models\n",
    "    in order to minimize the adjusted loss function and prevent overfitting or underfitting. \n",
    "    \n",
    "    It is the process of regularizing the parameters that constrain, regularizes, or shrinks\n",
    "    the coefficient estimates towards zero and by this it fit the data and reduces the error.\n",
    "    \n",
    "    two types of regularization techniques :\n",
    "    1. lasso regularization : It adds penalty term to the cost function. This term is the \n",
    "    absolute sum of the coefficients. As the value of coefficients increases from 0 this \n",
    "    term penalizes, cause model, to decrease the value of coefficients in order to reduce\n",
    "    loss.\n",
    "    \n",
    "    2.Ridge regularization - In this technique we add a penalty term which is equal to the\n",
    "    square of the coefficient. The L2 term is equal to the square of the magnitude of the \n",
    "    coefficients. We also add a coefficient lambda to control that penalty term. \n",
    "    In this case if  \\lambda  is zero then the equation is the basic OLS else if  \n",
    "    lambda > 0 then it will add a constraint to the coefficient. As we increase the\n",
    "    value of \\lambda this constraint causes the value of the coefficient to tend towards zero.\n",
    "    This leads to tradeoff of higher bias (dependencies on certain coefficients tend to be \n",
    "    0 and on certain coefficients tend to be very large, making the model less flexible) for\n",
    "    lower variance.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
